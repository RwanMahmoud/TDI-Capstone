{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import requests\n",
    "import dill # to store \n",
    "from bs4 import BeautifulSoup # for HTML scrapping \n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "from geopy.geocoders import Nominatim # module to convert an address into latitude and longitude values\n",
    "\n",
    "# libraries for displaying images\n",
    "from IPython.display import Image \n",
    "from IPython.core.display import HTML \n",
    "    \n",
    "# tranforming json file into a pandas dataframe library\n",
    "from pandas.io.json import json_normalize\n",
    "import folium # plotting library\n",
    "from scipy.stats import zscore\n",
    "from math import radians, cos, sin, asin, sqrt,pi, sin, atan2,acos\n",
    "from sklearn import preprocessing\n",
    "import json # library to handle JSON files\n",
    "from shapely.geometry import shape, Point\n",
    "from collections import defaultdict\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading any previously stored dataframe and check the locations of the stored listings in order to add new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datframe1.txt\", \"rb\") as fp:   # Unpickling\n",
    "    dff= pickle.load(fp)\n",
    "\n",
    "locs=[l.strip() for l in dff['location']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrapping the website to find the available listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using BeautifulDoup\n",
    "url= 'https://www.spacelist.ca/listings/on/toronto/retail/for-lease'\n",
    "page =  requests.get(url) # Use requests.get to download the page.\n",
    "soup = BeautifulSoup(page.text,'html.parser')\n",
    "\n",
    "listings=[]\n",
    "for element in soup.find_all('div', class_='meta-card'):\n",
    "    listings.append(element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeate for all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,11):\n",
    "    url= f'https://www.spacelist.ca/listings/on/toronto/retail/for-lease/page/{i}'\n",
    "    page =  requests.get(url) # Use requests.get to download the page.\n",
    "    soup = BeautifulSoup(page.text,'html.parser')\n",
    "\n",
    "    for element in soup.find_all('div', class_='meta-card'):\n",
    "        listings.append(element)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the datframe: by extracting the location, lease rate and the space of each listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces=[]\n",
    "space_idx=[]\n",
    "rents=[]\n",
    "rent_idx=[]\n",
    "locations=[]\n",
    "\n",
    "for listing in listings:\n",
    "    rent= listing.find('div',class_='cell shrink display-price')\n",
    "    space=listing.find('div',class_='cell auto about')\n",
    "    location=listing.find('div',class_='cell auto small-font dark-font flat-line-height right-cell-padding')\n",
    "    if type(rent) != type(None)  and type(space) != type(None) and type(location) != type(None) :\n",
    "    \n",
    "        text2= space.text\n",
    "        text=rent.text\n",
    "        text3=location.text\n",
    "    \n",
    "        if ',' in text:text=text.replace(',','')\n",
    "        if 'Sale' in text:text=text.replace('Sale','')\n",
    "        if '$' in text:text=text.replace('$','')\n",
    "        if ',' in text:text=text.replace(',','')\n",
    "        if '-' in text:text=text.split('-')[-1].strip()\n",
    "        #take the monthly rate only and handling the different formats of text\n",
    "        \n",
    "        if 'yr' in text:text= text.split('yr')[-1].strip()\n",
    "        if '/mo' in text: text= text.split('/mo')[0].strip()\n",
    "        if ',' in text2:text2=text2.replace(',','')\n",
    "        if '-' in text2:\n",
    "            space_ft2=text2.split('-')[0].strip()   \n",
    "        elif ('ft' in text2):\n",
    "            space_ft2=text2.split('ft')[0].strip()\n",
    "        elif('Desk'in text2):\n",
    "            space_ft2='25'\n",
    "            \n",
    "        if '-'in text3:place=text3.split('-')[-1]\n",
    "        else: place=text3\n",
    "        if place.strip() in locs:continue\n",
    "        rents.append(text)\n",
    "        spaces.append(space_ft2)\n",
    "        locations.append(place)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### removing data with no price info and finding the price per sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(rents,spaces,locations)), columns =['rent','spaces','location']) \n",
    "df=df.drop(df[df['rent']=='ContactContactLease'].index)\n",
    "df=df.drop(df[df['rent']=='ContactContactLease '].index)\n",
    "df=df.drop( df[df['rent']=='ContactContactLease (NNN)'].index ) \n",
    "df=df.drop( df[df['rent']=='ContactContactLease (Net)'].index ) \n",
    "df=df.drop(df[df['rent']=='ContactContactSublease'].index)\n",
    "df=df.drop(df[df['rent']=='ContactContactSublease '].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index(drop=True)\n",
    "df['price_per_sqft']=df['rent'].astype(float)/df['spaces'].astype(float)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### adding long and lat info for all the listings using geolocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latitude'] = \"\"\n",
    "df['longitude']=\"\"\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    address = df['location'][i]+ ', Toronto, canada'\n",
    "    geolocator = Nominatim(user_agent=\"ny_explorer\",timeout=3)\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        df['latitude'][i]= location.latitude\n",
    "        df['longitude'][i] = location.longitude\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding subway stations information from City of Toronto API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subway=pd.read_csv('TorontoSubwayStations_Ridership.csv')\n",
    "subway.rename(columns={\"y\": \"latitude\", \"x\": \"longitude\",\n",
    "                       \"Station Name\": \"station_name\",\n",
    "                       \"Total Daily Riders\":\"daily_riders\"}, errors=\"raise\", inplace=True)\n",
    "subway=subway.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = subway[['daily_riders']].values.astype(float)\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(1, 10))\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "subway['scaled_riders']=x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### distacne calculator function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    dLat = radians(lat2 - lat1)\n",
    "    dLon = radians(lon2 - lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "    phi=(lat1+lat2)/2\n",
    "\n",
    "    a = (dLat**2)+(cos(phi)*dLon)**2\n",
    "    c = (sqrt(a))\n",
    "\n",
    "    return (R * c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### finding the distance between each listing to the anearest subway station and adding it to the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nearest_subway']=\"\"\n",
    "df['dist_to_subway_km'] = \"\"\n",
    "\n",
    "for i in range(len(df)):\n",
    "    distances=[]\n",
    "    for lat,long in zip(subway['latitude'], subway['longitude']):\n",
    "        distances.append(distance(df['latitude'][i],df['longitude'][i],lat,long))\n",
    "        \n",
    "    x=min(distances)\n",
    "    for j in range(len(distances)):\n",
    "        if distances[j]==x:\n",
    "            nearest=subway.iloc[j]['station_name']\n",
    "            daily_riders=subway.iloc[j]['daily_riders']\n",
    "\n",
    "    df['dist_to_subway_km'][i]=min(distances)\n",
    "    df['nearest_subway'][i]=nearest\n",
    "    df['subway_daily_riders']=daily_riders\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### finding the distance to the nearest bus stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops=pd.read_csv('stops.csv')\n",
    "df['dist_to_bus_km'] = \"\"\n",
    "\n",
    "for i in range(len(df)):\n",
    "    distances=[]\n",
    "    for lat,long in zip(stops['stop_lat'], stops['stop_lon']):\n",
    "        distances.append(distance(df['latitude'][i],df['longitude'][i],lat,long))\n",
    "\n",
    "    df['dist_to_bus_km'][i]=min(distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we find the neihbourhood of each listing to add the neihbourhood information to the data frame using shapely.geometry library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a geojson file that has the bounderies of each neihbourhood\n",
    "with open('Neighbourhoods.geojson.json') as f:\n",
    "    js = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=[]\n",
    "for ind in df.index: \n",
    "    point= Point(df['longitude'][ind],df['latitude'][ind])\n",
    "    for feature in js['features']:\n",
    "        polygon = shape(feature['geometry'])\n",
    "        if polygon.contains(point):\n",
    "                codes.append(feature['properties']['AREA_SHORT_CODE'])\n",
    "\n",
    "df['AREA_SHORT_CODE']=codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more info about each neighborhood will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'population10_89.csv'\n",
    "population = pd.read_csv(file)\n",
    "population.drop('Combined Indicators',1,inplace=True)\n",
    "file = 'economics _toronto.csv'\n",
    "economics = pd.read_csv(file)\n",
    "economics.drop('Combined Indicators',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Neighbourhood_Crime_Rates_Boundary_File_.csv'\n",
    "crime_data = pd.read_csv(file)\n",
    "columns=['Hood_ID','Neighbourhood','Assault_AVG','Assault_CHG','AutoTheft_AVG','AutoTheft_CHG','BreakandEnter_AVG','BreakandEnter_CHG','Robbery_AVG','Robbery_CHG','heftOver_AVG','TheftOver_CHG','Homicide_AVG','Homicide_CHG','Population','Shape__Area','Shape__Length']\n",
    "crime=crime_data.loc[:,columns]\n",
    "data_df=population.merge(economics,  how='left', left_on=['Neighbourhood Id','Neighbourhood'],\n",
    "                         right_on=['Neighbourhood Id','Neighbourhood'])\n",
    "data_df=data_df.merge(crime,how='left', left_on=['Neighbourhood Id','Neighbourhood'],\n",
    "                      right_on=['Hood_ID','Neighbourhood'])\n",
    "columns=['Assault_CHG','AutoTheft_CHG','Hood_ID','BreakandEnter_CHG','Robbery_CHG','Homicide_AVG','Population','heftOver_AVG','TheftOver_CHG','Homicide_CHG']\n",
    "data_df.drop(columns,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grouping the different age populations into 4 age groups that could be relevent to the different businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_pop=[]\n",
    "Pop_10_24=[]\n",
    "Pop_25_39=[]\n",
    "Pop_40_59=[]\n",
    "Pop_60_89=[]\n",
    "Pop_Males=[]\n",
    "Pop_Females=[]\n",
    "avg_crime=[]\n",
    "Low_Income_Families=[]\n",
    "Part_Time_Employment=[]\n",
    "shape_area=[]\n",
    "for i in range(len(df)):\n",
    "    Nid=int(df.iloc[i]['AREA_SHORT_CODE'])\n",
    "    total_pop.append(data_df[data_df['Neighbourhood Id']==Nid]['Total Population'].values[0])\n",
    "    Pop_10_24.append(data_df[data_df['Neighbourhood Id']==Nid]['Pop 10 - 14 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 15 - 19 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 20 - 24 years'].values[0])\n",
    "    Pop_25_39.append(data_df[data_df['Neighbourhood Id']==Nid]['Pop 25 - 29 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 30 - 34 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 35 - 39 years'].values[0])\n",
    "    Pop_40_59.append(data_df[data_df['Neighbourhood Id']==Nid]['Pop 40 - 44 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 45 - 49 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 50 - 54 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 55 - 59 years'].values[0])\n",
    "    Pop_60_89.append(data_df[data_df['Neighbourhood Id']==Nid]['Pop 60 - 64 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 65 - 69 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 70 - 74 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 75 - 79 years'].values[0] +data_df[data_df['Neighbourhood Id']==Nid]['Pop 80 - 84 years'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Pop 85 - 89 years'].values[0])\n",
    "    \n",
    "    Pop_Males.append(data_df[data_df['Neighbourhood Id']==Nid]['Pop - Males'].values[0])\n",
    "    Pop_Females.append(data_df[data_df['Neighbourhood Id']==Nid]['Pop - Females'].values[0])\n",
    "    avg_crime.append(data_df[data_df['Neighbourhood Id']==Nid]['AutoTheft_AVG'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['BreakandEnter_AVG'].values[0]+data_df[data_df['Neighbourhood Id']==Nid]['Robbery_AVG'].values[0])\n",
    "    \n",
    "    Low_Income_Families.append(data_df[data_df['Neighbourhood Id']==Nid]['Low Income Families'].values[0])\n",
    "    Part_Time_Employment.append(data_df[data_df['Neighbourhood Id']==Nid]['Part-Time Employment'].values[0])\n",
    "    shape_area.append(data_df[data_df['Neighbourhood Id']==Nid]['Shape__Area'].values[0])\n",
    "    \n",
    "df['Total_Population']=total_pop\n",
    "df['Pop_10-24']=Pop_10_24\n",
    "df['Pop_25-39']=Pop_25_39\n",
    "df['Pop_40-59']=Pop_40_59\n",
    "df['Pop_60-89']=Pop_60_89\n",
    "df['Pop-Males']=Pop_Males\n",
    "df['Pop-Females']=Pop_Females\n",
    "df['avg_crime']=avg_crime\n",
    "df['Low_Income_Families']=Low_Income_Families\n",
    "df['Part_Time_Employment']=Part_Time_Employment\n",
    "df['Neighbor_area']=shape_area\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now exploring sourounding venues using Foursqure API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### finding nearby cafes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Foursquare API\n",
    "CLIENT_ID =  # your Foursquare ID\n",
    "CLIENT_SECRET = # your Foursquare Secret\n",
    "VERSION = '20200202'\n",
    "LIMIT = 10000\n",
    "radius = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'cafe'\n",
    "radius = 500\n",
    "cafe=[]\n",
    "for i in range(len(df)):\n",
    "    latitude=df.iloc[i]['latitude']\n",
    "    longitude=df.iloc[i]['longitude']\n",
    "    # forsquare data\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n",
    "    results = requests.get(url).json()\n",
    "    venues = results['response']['venues']\n",
    "    cafe.append(len(venues))\n",
    "df['cafes_within_500m']=cafe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding nearby colleges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'college'\n",
    "radius = 1000\n",
    "college=[]\n",
    "for i in range(len(df)):\n",
    "    latitude=df.iloc[i]['latitude']\n",
    "    longitude=df.iloc[i]['longitude']\n",
    "    # forsquare data\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n",
    "    results = requests.get(url).json()\n",
    "    venues = results['response']['venues']\n",
    "    if venues :college.append(len(venues))\n",
    "    else: college.append(0)\n",
    "    \n",
    "df['colleges_within_1000m']=college\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding nearby restaurants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'food'\n",
    "radius = 500\n",
    "food=[]\n",
    "for i in range(len(df)):\n",
    "    latitude=df.iloc[i]['latitude']\n",
    "    longitude=df.iloc[i]['longitude']\n",
    "    # forsquare data\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n",
    "    results = requests.get(url).json()\n",
    "    venues = results['response']['venues']\n",
    "    if venues :food.append(len(venues))\n",
    "    else: food.append(0)\n",
    "    \n",
    "df['food_within_500m']=food\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding nearby Medical centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'Medical Center'\n",
    "radius = 1000\n",
    "hospital=[]\n",
    "for i in range(len(df)):\n",
    "    latitude=df.iloc[i]['latitude']\n",
    "    longitude=df.iloc[i]['longitude']\n",
    "    # forsquare data\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n",
    "    results = requests.get(url).json()\n",
    "    venues = results['response']['venues']\n",
    "    if venues :hospital.append(len(venues))\n",
    "    else: hospital.append(0)\n",
    "    \n",
    "df['hospitals_within_1000m']=hospital\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding nearby Shopping centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'shopping'\n",
    "radius = 1500\n",
    "shopping=[]\n",
    "for i in range(len(df)):\n",
    "    latitude=df.iloc[i]['latitude']\n",
    "    longitude=df.iloc[i]['longitude']\n",
    "    # forsquare data\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n",
    "    results = requests.get(url).json()\n",
    "    venues = results['response']['venues']\n",
    "    if venues :shopping.append(len(venues))\n",
    "    else: shopping.append(0)\n",
    "    \n",
    "df['shopping_within_1500m']=shopping\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding nearby fun centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'fun'\n",
    "radius = 500\n",
    "fun=[]\n",
    "for i in range(len(df)):\n",
    "    latitude=df.iloc[i]['latitude']\n",
    "    longitude=df.iloc[i]['longitude']\n",
    "    # forsquare data\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n",
    "    results = requests.get(url).json()\n",
    "    venues = results['response']['venues']\n",
    "    if venues :fun.append(len(venues))\n",
    "    else: fun.append(0)\n",
    "    \n",
    "df['fun_within_500m']=fun\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring surrounding venues to find the average ratings, no. of check-ins, and  tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyids(latitude, longitude, radius=200):\n",
    "    \"\"\"\n",
    "    function that returns a list of venues and their ids \n",
    "    within a specific radius from a given lat and long\n",
    "    \"\"\"\n",
    "    venues_list=[]\n",
    "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            latitude, \n",
    "            longitude, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "     \n",
    "    results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "    return [v['venue']['id'] for v in results]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_venue_info(ids,venues):\n",
    "    \"\"\"\n",
    "    this function adds venue information(tips, rating) to a list of venues\n",
    "    \"\"\"\n",
    "    rating_sum=0\n",
    "    tips_sum=0\n",
    "    for vid in ids:\n",
    "        if vid not in venues.keys():\n",
    "            url='https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(vid, CLIENT_ID, CLIENT_SECRET, VERSION)\n",
    "            results = requests.get(url).json()\n",
    "            if 'rating' in results['response']['venue'].keys():\n",
    "                rating=results['response']['venue']['rating']\n",
    "            else: rating=0\n",
    "            tips=results['response']['venue']['stats']['tipCount']\n",
    "            rating_sum+=rating\n",
    "            tips_sum+=tips\n",
    "            venues[vid]=[rating,tips]\n",
    "    return venues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listing_nearby(ids,D):\n",
    "    \"\"\"\n",
    "    return the average of ratings and total tips\n",
    "    \"\"\"\n",
    "    rating_sum=0\n",
    "    tips_sum=0\n",
    "    for vid in ids:\n",
    "        rating_sum+=D[vid][0]\n",
    "        tips_sum+=D[vid][1]\n",
    "    \n",
    "    if len(ids)>0:return(rating_sum/len(ids),tips_sum)\n",
    "    else: return(0,0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-df968602c8c7>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-df968602c8c7>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    CLIENT_ID =  # your Foursquare ID\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##Foursquare API\n",
    "CLIENT_ID =  # your Foursquare ID\n",
    "CLIENT_SECRET =  # your Foursquare Secret\n",
    "VERSION = '20200202'\n",
    "LIMIT = 10000\n",
    "radius = 150\n",
    "# to avoid checking the already visited venues we use dictionar to store list of venues ids \n",
    "IDs = defaultdict(list) \n",
    "no_nearby_venues=[]\n",
    "no_of_tips=[]\n",
    "avg_rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "#for i in range(0,10):\n",
    "    print(i)\n",
    "    #get the nearby venues ids, within 500m\n",
    "    ids=getNearbyids(df.iloc[i]['latitude'],df.iloc[i]['longitude'],radius)\n",
    "    ## for each veneue get the number of tips and the rating\n",
    "    IDs=get_venue_info(ids,IDs)\n",
    "    ##for each listing get the average rating for the sorrounding venues and the total number of tips\n",
    "    rating,tips=listing_nearby(ids,IDs)\n",
    "    no_nearby_venues.append(len(ids))\n",
    "    avg_rating.append(rating)\n",
    "    no_of_tips.append(tips)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding this information to the datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_nearby_venues']=no_nearby_venues\n",
    "df['avg_rating']=avg_rating\n",
    "df['no_of_tips']=no_of_tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combining the old anf the new datframes and store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [dff, df]\n",
    "result = pd.concat(frames);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"datframe1.txt\", \"wb\") as fp:\n",
    "#    pickle.dump(result, fp)\n",
    "\n",
    "with open(\"datframe1.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df= pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rent']=df['rent'].astype(float)\n",
    "df['spaces']=df['spaces'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy=df.dropna()\n",
    "#dropping irrelevant features\n",
    "X=data_copy.drop(columns=['price_per_sqft',\n",
    "                            'AREA_SHORT_CODE',\n",
    "                          'rent','nearest_subway','location'\n",
    "                         ])\n",
    "# Features \n",
    "X=X.reset_index(drop=True)\n",
    "# Targets\n",
    "y=data_copy['price_per_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Low_Income_Families', 'Neighbor_area', 'Part_Time_Employment',\n",
       "       'Pop-Females', 'Pop-Males', 'Pop_10-24', 'Pop_25-39', 'Pop_40-59',\n",
       "       'Pop_60-89', 'Total_Population', 'avg_crime', 'avg_rating',\n",
       "       'cafes_within_500m', 'colleges_within_1000m', 'dist_to_bus_km',\n",
       "       'dist_to_subway_km', 'food_within_500m', 'fun_within_500m',\n",
       "       'hospitals_within_1000m', 'latitude', 'longitude', 'no_nearby_venues',\n",
       "       'no_of_tips', 'shopping_within_1500m', 'spaces', 'subway_daily_riders'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(X.columns))\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using MinMaxScaler to scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=0.33)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_scaled=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35598244618063146"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est= GradientBoostingRegressor(random_state=0)\n",
    "grid_values={'learning_rate': [i for i in np.linspace(0.001, 0.1, num=100)],\n",
    "              'n_estimators':[int(i) for i in np.linspace(51, 150, num=100)]\n",
    "             }\n",
    "clf=GridSearchCV(est, param_grid = grid_values,cv=4).fit(X_scaled,y)\n",
    "\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid best for forest={'learning_rate': 0.056, 'n_estimators': 52} \n"
     ]
    }
   ],
   "source": [
    "print(f'Grid best for forest={clf.best_params_} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to study the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "est= GradientBoostingRegressor(learning_rate= 0.056, n_estimators= 52).fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cafes_within_500m',\n",
       " 'dist_to_subway_km',\n",
       " 'spaces',\n",
       " 'colleges_within_1000m',\n",
       " 'longitude',\n",
       " 'avg_rating',\n",
       " 'no_nearby_venues',\n",
       " 'no_of_tips',\n",
       " 'avg_crime',\n",
       " 'latitude',\n",
       " 'Low_Income_Families',\n",
       " 'dist_to_bus_km',\n",
       " 'Pop_60-89',\n",
       " 'Pop_25-39',\n",
       " 'hospitals_within_1000m',\n",
       " 'food_within_500m',\n",
       " 'Pop-Males',\n",
       " 'shopping_within_1500m',\n",
       " 'Neighbor_area',\n",
       " 'fun_within_500m',\n",
       " 'Pop_10-24',\n",
       " 'Part_Time_Employment',\n",
       " 'Pop-Females',\n",
       " 'Pop_40-59',\n",
       " 'Total_Population',\n",
       " 'subway_daily_riders']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for _, x in sorted(zip(est.feature_importances_,X.columns), key=lambda pair: -pair[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=RidgeCV(alphas=np.logspace(-0.5,0.6,150))\n",
    "scores = cross_val_score(ridge, X_scaled, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est =DecisionTreeRegressor(random_state=0)\n",
    "grid_values={'min_samples_leaf': [int(i) for i in np.linspace(1, 10, num=10)],\n",
    "              'max_depth':[int(i) for i in np.linspace(1, 20, num=20)]\n",
    "             }\n",
    "\n",
    "clf=GridSearchCV(est, param_grid = grid_values,cv=4).fit(X_scaled,y)\n",
    "\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_values={'min_samples_leaf': [int(i) for i in np.linspace(1, 10, num=10)],\n",
    "              'max_depth':[int(i) for i in np.linspace(1, 20, num=20)]\n",
    "             }\n",
    "est=RandomForestRegressor(n_estimators=10,random_state=0)\n",
    "clf=GridSearchCV(est, param_grid = grid_values,cv=5).fit(X_scaled,y)\n",
    "\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Grid best for forest={clf.best_params_} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(hidden_layer_sizes=(10,10,100),random_state=0)\n",
    "scores = cross_val_score(clf, X_scaled, y, cv=4)\n",
    "scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
